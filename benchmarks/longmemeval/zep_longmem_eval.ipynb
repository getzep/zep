{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i9i9uUZ3pWQE",
    "outputId": "0e770b5d-705f-4a25-b4b6-7cb613e149a2"
   },
   "outputs": [],
   "source": [
    "######## Installations\n",
    "\n",
    "!pip install zep-cloud openai gdown\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgzveXyAp35v"
   },
   "outputs": [],
   "source": [
    "######## Imports\n",
    "\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import gdown\n",
    "import json\n",
    "import tarfile\n",
    "from zep_cloud.client import AsyncZep\n",
    "from zep_cloud import Message\n",
    "from openai import AsyncOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from pydantic import BaseModel, Field\n",
    "from zep_cloud import EntityEdge, EntityNode\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iwcBxgxgqES8",
    "outputId": "455c833c-b437-4b16-f20c-ef2bede96c04"
   },
   "outputs": [],
   "source": [
    "######## Download the eval dataset from the official Google Drive source\n",
    "\n",
    "file_id = \"1zJgtYRFhOh5zDQzzatiddfjYhFSnyQ80\"\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "file_path = \"longmemeval_data.tar.gz\"\n",
    "\n",
    "# Download the compressed dataset\n",
    "if not os.path.exists(file_path):\n",
    "    gdown.download(url, file_path, quiet=False)\n",
    "else:\n",
    "    print(f\"'{file_path}' already exists, skipping download.\")\n",
    "\n",
    "# Extract the tar.gz\n",
    "if not os.path.exists(\"./longmemeval_oracle.json\"):\n",
    "    with tarfile.open(file_path, \"r:gz\") as tar:\n",
    "        tar.extractall()\n",
    "else:\n",
    "    print(\"'longmemeval_oracle.json' already exists, so skipping extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_RjEZnk5v530"
   },
   "outputs": [],
   "source": [
    "######## Load the eval dataset\n",
    "\n",
    "lme_dataset_option = \"data/longmemeval_s.json\"  # Can be _oracle, _s, or _m\n",
    "lme_dataset_df = pd.read_json(lme_dataset_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "kZ9MYQSXw-VF",
    "outputId": "b5c3c0fa-974f-4621-efc0-8b0c72741da5"
   },
   "outputs": [],
   "source": [
    "######## (Optionally skip this cell) Display some rows of the dataframe\n",
    "\n",
    "lme_dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FG9nej7kwHIr"
   },
   "outputs": [],
   "source": [
    "######## Start up Zep and OpenAI clients (make sure you set these keys in Colab's secrets tab in the left sidebar)\n",
    "load_dotenv()\n",
    "\n",
    "# , base_url=\"https://api.development.getzep.com/api/v2\"\n",
    "\n",
    "zep = AsyncZep(\n",
    "    api_key=os.getenv(\"ZEP_API_KEY\"),\n",
    "    base_url=\"https://api.development.getzep.com/api/v2\",\n",
    ")\n",
    "oai_client = AsyncOpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qy7mIV5t3pD"
   },
   "outputs": [],
   "source": [
    "######## LongMemEval  - ingest each multi-session as its own single Zep user/session pair\n",
    "\n",
    "num_multi_sessions = 500\n",
    "\n",
    "\n",
    "for multi_session_idx in range(num_multi_sessions):\n",
    "    multi_session = lme_dataset_df[\"haystack_sessions\"].iloc[multi_session_idx]\n",
    "    multi_session_dates = lme_dataset_df[\"haystack_dates\"].iloc[multi_session_idx]\n",
    "\n",
    "    question_type = lme_dataset_df[\"question_type\"][multi_session_idx]\n",
    "\n",
    "    if question_type != \"single-session-assistant\":\n",
    "        continue\n",
    "\n",
    "    print(question_type)\n",
    "\n",
    "    # Create a unique Zep user and session for this multi-session.\n",
    "    # We only use one Zep session because it doesn't change things to use multiple Zep sessions\n",
    "    user_id = \"lme_s_experiment_user_\" + str(multi_session_idx)\n",
    "    session_id = \"lme_s_experiment_session_\" + str(multi_session_idx)\n",
    "\n",
    "    # Uncomment this code to delete existing users\n",
    "    # try:\n",
    "    #     await zep.user.delete(user_id)\n",
    "    #     await zep.memory.delete(session_id)\n",
    "    # except:\n",
    "    #     pass\n",
    "    #\n",
    "    # continue\n",
    "\n",
    "    await zep.user.add(user_id=user_id)\n",
    "    await zep.memory.add_session(\n",
    "        user_id=user_id,\n",
    "        session_id=session_id,\n",
    "    )\n",
    "\n",
    "    for session_idx, session in enumerate(multi_session):\n",
    "        for msx_idx, msg in enumerate(session):\n",
    "            date = multi_session_dates[session_idx] + \" UTC\"\n",
    "            date_format = \"%Y/%m/%d (%a) %H:%M UTC\"\n",
    "            date_string = datetime.strptime(date, date_format).replace(\n",
    "                tzinfo=timezone.utc\n",
    "            )\n",
    "            await zep.memory.add(\n",
    "                session_id=session_id,\n",
    "                messages=[\n",
    "                    Message(\n",
    "                        role=msg[\"role\"],\n",
    "                        role_type=msg[\"role\"],\n",
    "                        content=msg[\"content\"][:8000],\n",
    "                        created_at=date_string.isoformat(),\n",
    "                    )\n",
    "                ],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Define prompts for LongMemEval (LME) eval\n",
    "async def lme_response(llm_client, context: str, question: str) -> str:\n",
    "    system_prompt = \"\"\"\n",
    "        You are a helpful expert assistant answering questions from lme_experiment users based on the provided context.\n",
    "        \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "            Your task is to briefly answer the question. You are given the following context from the previous conversation. If you don't know how to answer the question, abstain from answering.\n",
    "                <CONTEXT>\n",
    "                {context}\n",
    "                </CONTEXT>\n",
    "                <QUESTION>\n",
    "                {question}\n",
    "                </QUESTION>\n",
    "\n",
    "            Answer:\n",
    "            \"\"\"\n",
    "\n",
    "    response = await llm_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    result = response.choices[0].message.content or \"\"\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class Grade(BaseModel):\n",
    "    is_correct: str = Field(description=\"yes or no\")\n",
    "\n",
    "\n",
    "async def lme_grader(\n",
    "    llm_client, question: str, gold_answer: str, response: str, question_type: str\n",
    ") -> bool:\n",
    "    system_prompt = \"\"\"\n",
    "        You are an expert grader that determines if answers to questions match a gold standard answer\n",
    "        \"\"\"\n",
    "\n",
    "    TEMPORAL_REASONING_PROMPT = f\"\"\"\n",
    "    I will give you a question, a correct answer, and a response from a model. Please answer yes if the response contains the correct answer. Otherwise, answer no. If the response is equivalent to the correct answer or contains all the intermediate steps to get the correct answer, you should also answer yes. If the response only contains a subset of the information required by the answer, answer no. In addition, do not penalize off-by-one errors for the number of days. If the question asks for the number of days/weeks/months, etc., and the model makes off-by-one errors (e.g., predicting 19 days when the answer is 18), the model’s response is still correct.\n",
    "\n",
    "    <QUESTION>\n",
    "    B: {question}\n",
    "    </QUESTION>\n",
    "    <CORRECT ANSWER>\n",
    "    {gold_answer}\n",
    "    </CORRECT ANSWER>\n",
    "    <RESPONSE>\n",
    "    A: {response}\n",
    "    </RESPONSE>\n",
    "    \"\"\"\n",
    "\n",
    "    KNOWLEDGE_UPDATE_PROMPT = f\"\"\"\n",
    "    I will give you a question, a correct answer, and a response from a model. Please answer yes if the response contains the correct answer. Otherwise, answer no. If the response contains some previous information along with an updated answer, the response should be considered as correct as long as the updated answer is the required answer.\n",
    "    \n",
    "    <QUESTION>\n",
    "    B: {question}\n",
    "    </QUESTION>\n",
    "    <CORRECT ANSWER>\n",
    "    {gold_answer}\n",
    "    </CORRECT ANSWER>\n",
    "    <RESPONSE>\n",
    "    A: {response}\n",
    "    </RESPONSE>\n",
    "    \"\"\"\n",
    "\n",
    "    SINGLE_SESSION_PREFERENCE = f\"\"\"\n",
    "    I will give you a question, a rubric for desired personalized response, and a response from a model. Please answer yes if the response satisfies the desired response. Otherwise, answer no. The model does not need to reflect all the points in the rubric. The response is correct as long as it recalls and utilizes the user’s personal information correctly.\n",
    "    \n",
    "    <QUESTION>\n",
    "    B: {question}\n",
    "    </QUESTION>\n",
    "    <RUBRIC>\n",
    "    {gold_answer}\n",
    "    </RUBRIC>\n",
    "    <RESPONSE>\n",
    "    A: {response}\n",
    "    </RESPONSE>\n",
    "    \"\"\"\n",
    "\n",
    "    DEFAULT_PROMPT = f\"\"\"         \n",
    "    I will give you a question, a correct answer, and a response from a model. Please answer yes if the response contains the correct answer. Otherwise, answer no. If the response is equivalent to the correct answer or contains all the intermediate steps to get the correct answer, you should also answer yes. If the response only contains a subset of the information required by the answer, answer no.\n",
    "            \n",
    "    <QUESTION>\n",
    "    B: {question}\n",
    "    </QUESTION>\n",
    "    <CORRECT ANSWER>\n",
    "    {gold_answer}\n",
    "    </CORRECT ANSWER>\n",
    "    <RESPONSE>\n",
    "    A: {response}\n",
    "    </RESPONSE>\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = DEFAULT_PROMPT\n",
    "    if question_type == \"temporal-reasoning\":\n",
    "        prompt = TEMPORAL_REASONING_PROMPT\n",
    "    elif question_type == \"knowledge-update\":\n",
    "        prompt = KNOWLEDGE_UPDATE_PROMPT\n",
    "    elif question_type == \"single-session-preference\":\n",
    "        prompt = SINGLE_SESSION_PREFERENCE\n",
    "\n",
    "    response = await llm_client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        response_format=Grade,\n",
    "        temperature=0,\n",
    "    )\n",
    "    result = response.choices[0].message.parsed\n",
    "\n",
    "    return result.is_correct.strip().lower() == \"yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PuBYkFA_YEIN",
    "outputId": "e697bc9a-22b4-44aa-eb0a-28acf568f096"
   },
   "outputs": [],
   "source": [
    "######## LongMemEval  - evaluation\n",
    "\n",
    "zep_answers_file_path = \"./longmemeval_zep_answers.jsonl\"\n",
    "\n",
    "TEMPLATE = \"\"\"\n",
    "FACTS and ENTITIES represent relevant context to the current conversation.\n",
    "\n",
    "# These are the most relevant facts and their valid date ranges. If the fact is about an event, the event takes place during this time.\n",
    "# format: FACT (Date range: from - to)\n",
    "<FACTS>\n",
    "{facts}\n",
    "</FACTS>\n",
    "\n",
    "# These are the most relevant entities\n",
    "# ENTITY_NAME: entity summary\n",
    "<ENTITIES>\n",
    "{entities}\n",
    "</ENTITIES>\n",
    "\"\"\"\n",
    "\n",
    "total_duration = 0\n",
    "\n",
    "\n",
    "def format_edge_date_range(edge: EntityEdge) -> str:\n",
    "    # return f\"{datetime(edge.valid_at).strftime('%Y-%m-%d %H:%M:%S') if edge.valid_at else 'date unknown'} - {(edge.invalid_at.strftime('%Y-%m-%d %H:%M:%S') if edge.invalid_at else 'present')}\"\n",
    "    return f\"{edge.valid_at if edge.valid_at else 'date unknown'} - {(edge.invalid_at if edge.invalid_at else 'present')}\"\n",
    "\n",
    "\n",
    "def compose_search_context(edges: list[EntityEdge], nodes: list[EntityNode]) -> str:\n",
    "    facts = [f\"  - {edge.fact} ({format_edge_date_range(edge)})\" for edge in edges]\n",
    "    entities = [f\"  - {node.name}: {node.summary}\" for node in nodes]\n",
    "    return TEMPLATE.format(facts=\"\\n\".join(facts), entities=\"\\n\".join(entities))\n",
    "\n",
    "\n",
    "async def evaluate_conversation(multi_session_idx) -> tuple[int, float, float]:\n",
    "    # Set user values\n",
    "    user_id = \"lme_s_experiment_user_\" + str(multi_session_idx)\n",
    "    session_id = \"lme_s_experiment_session_\" + str(multi_session_idx)\n",
    "\n",
    "    # Now we want to prompt an LLM augmented with Zep memory to answer the question\n",
    "    question_id = lme_dataset_df[\"question_id\"][multi_session_idx]\n",
    "    question_type = lme_dataset_df[\"question_type\"][multi_session_idx]\n",
    "\n",
    "    question = (\n",
    "        \"(date: \"\n",
    "        + lme_dataset_df[\"question_date\"][multi_session_idx]\n",
    "        + \") \"\n",
    "        + lme_dataset_df[\"question\"][multi_session_idx]\n",
    "    )\n",
    "    gold_answer = lme_dataset_df[\"answer\"][multi_session_idx]\n",
    "\n",
    "    # Get relevant facts and entities\n",
    "    start = time()\n",
    "    edges_results = (\n",
    "        await zep.graph.search(\n",
    "            user_id=user_id,\n",
    "            reranker=\"cross_encoder\",\n",
    "            query=question[0:255],\n",
    "            scope=\"edges\",\n",
    "            limit=20,\n",
    "        )\n",
    "    ).edges\n",
    "    node_results = (\n",
    "        await zep.graph.search(\n",
    "            user_id=user_id,\n",
    "            reranker=\"rrf\",\n",
    "            query=question[0:255],\n",
    "            scope=\"nodes\",\n",
    "            limit=20,\n",
    "        )\n",
    "    ).nodes\n",
    "    retrieval_duration = time() - start\n",
    "\n",
    "    context = compose_search_context(edges_results, node_results)\n",
    "    context_len = len(context.split(\" \"))\n",
    "\n",
    "    # Prompt an LLM with relevant context\n",
    "    hypothesis = await lme_response(oai_client, context, question)\n",
    "    duration = time() - start\n",
    "\n",
    "    grade = await lme_grader(\n",
    "        oai_client, question, gold_answer, hypothesis, question_type\n",
    "    )\n",
    "\n",
    "    zep_answers.append(\n",
    "        {\n",
    "            \"question_id\": question_id,\n",
    "            \"hypothesis\": hypothesis,\n",
    "            \"gold_answer\": gold_answer,\n",
    "            \"context\": context,\n",
    "            \"question_type\": question_type,\n",
    "            \"context_len\": context_len,\n",
    "            \"retrieval_duration\": retrieval_duration,\n",
    "            \"duration\": duration,\n",
    "            \"grade\": grade,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if grade:\n",
    "        return 1, duration, retrieval_duration\n",
    "    else:\n",
    "        return 0, duration, retrieval_duration\n",
    "\n",
    "\n",
    "default_results = {\"Number correct\": 0, \"Number total\": 0, \"Accuracy\": 0}\n",
    "question_type_set = set(lme_dataset_df[\"question_type\"].unique())\n",
    "eval_results = {\n",
    "    question_type: default_results.copy() for question_type in question_type_set\n",
    "}\n",
    "\n",
    "num_multi_sessions = 500\n",
    "\n",
    "zep_answers = []\n",
    "missed = []\n",
    "score = 0\n",
    "grades = []\n",
    "durations = []\n",
    "retrieval_durations = []\n",
    "\n",
    "idx_start = 0\n",
    "while idx_start < num_multi_sessions - 1:\n",
    "    print(idx_start)\n",
    "    results = await asyncio.gather(\n",
    "        *[\n",
    "            evaluate_conversation(multi_session_idx)\n",
    "            for multi_session_idx in range(idx_start, idx_start + 5)\n",
    "        ]\n",
    "    )\n",
    "    idx_start = idx_start + 5\n",
    "\n",
    "    for grade, duration, retrieval_duration in results:\n",
    "        grades.append(grade)\n",
    "        durations.append(duration)\n",
    "        retrieval_durations.append(retrieval_duration)\n",
    "\n",
    "with open(zep_answers_file_path, \"w\") as jsonl_file:\n",
    "    jsonl_file.write(json.dumps(zep_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(zep_answers_file_path, \"r\") as jsonl_file:\n",
    "    for i in range(500):\n",
    "        hypothesis = jsonl_file[i][\"hypothesis\"]\n",
    "        gold_answer = lme_dataset_df[\"answer\"][i]\n",
    "        print(hypothesis, gold_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## LongMemEval  - evaluation baseline\n",
    "baseline_answers_file_path = \"./longmemeval_baseline_answers.jsonl\"\n",
    "\n",
    "\n",
    "async def evaluate_conversation(multi_session_idx) -> tuple[int, float]:\n",
    "    # Now we want to prompt an LLM augmented with Zep memory to answer the question\n",
    "    question_id = lme_dataset_df[\"question_id\"][multi_session_idx]\n",
    "    question_type = lme_dataset_df[\"question_type\"][multi_session_idx]\n",
    "\n",
    "    question = (\n",
    "        \"(date: \"\n",
    "        + lme_dataset_df[\"question_date\"][multi_session_idx]\n",
    "        + \") \"\n",
    "        + lme_dataset_df[\"question\"][multi_session_idx]\n",
    "    )\n",
    "    gold_answer = lme_dataset_df[\"answer\"][multi_session_idx]\n",
    "\n",
    "    multi_session = lme_dataset_df[\"haystack_sessions\"].iloc[multi_session_idx]\n",
    "    multi_session_dates = lme_dataset_df[\"haystack_dates\"].iloc[multi_session_idx]\n",
    "\n",
    "    context = \"\"\n",
    "    for session_idx, session in enumerate(multi_session):\n",
    "        for msx_idx, msg in enumerate(session):\n",
    "            date = multi_session_dates[session_idx] + \" UTC\"\n",
    "            date_format = \"%Y/%m/%d (%a) %H:%M UTC\"\n",
    "            date_string = datetime.strptime(date, date_format).replace(\n",
    "                tzinfo=timezone.utc\n",
    "            )\n",
    "            context += f\"{msg['role']} (date: {date_string}): {msg['content']}\\n\"\n",
    "\n",
    "    start = time()\n",
    "    # Prompt an LLM with relevant context\n",
    "    hypothesis = await lme_response(oai_client, context, question)\n",
    "\n",
    "    duration = time() - start\n",
    "\n",
    "    grade = await lme_grader(\n",
    "        oai_client, question, gold_answer, hypothesis, question_type\n",
    "    )\n",
    "\n",
    "    baseline_answers.append(\n",
    "        {\n",
    "            \"question_id\": question_id,\n",
    "            \"hypothesis\": hypothesis,\n",
    "            \"gold_answer\": gold_answer,\n",
    "            \"context\": context,\n",
    "            \"question_type\": question_type,\n",
    "            \"retrieval_duration\": retrieval_duration,\n",
    "            \"duration\": duration,\n",
    "            \"grade\": grade,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if grade:\n",
    "        return 1, duration\n",
    "    else:\n",
    "        print(\"id:\", multi_session_idx, \"answer:\", hypothesis)\n",
    "        return 0, duration\n",
    "\n",
    "\n",
    "default_results = {\"Number correct\": 0, \"Number total\": 0, \"Accuracy\": 0}\n",
    "question_type_set = set(lme_dataset_df[\"question_type\"].unique())\n",
    "eval_results = {\n",
    "    question_type: default_results.copy() for question_type in question_type_set\n",
    "}\n",
    "\n",
    "num_multi_sessions = 500\n",
    "\n",
    "baseline_answers = []\n",
    "\n",
    "idx_start = 0\n",
    "while idx_start < num_multi_sessions - 1:\n",
    "    results = await asyncio.gather(\n",
    "        *[\n",
    "            evaluate_conversation(multi_session_idx)\n",
    "            for multi_session_idx in range(idx_start, idx_start + 100)\n",
    "        ]\n",
    "    )\n",
    "    idx_start = idx_start + 100\n",
    "\n",
    "    for curr_grade, curr_duration in results:\n",
    "        grades.append(curr_grade)\n",
    "        durations.append(curr_duration)\n",
    "\n",
    "    with open(zep_answers_file_path, \"w\") as jsonl_file:\n",
    "        jsonl_file.write(json.dumps(zep_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "collapsed": true,
    "id": "qWz5SxuDwR3p",
    "outputId": "6b403450-ab5b-449b-b464-1726b6740686"
   },
   "outputs": [],
   "source": [
    "######## (Optional dataset investigation) Loop through multi-sessions and record multi-session lengths and session lengths by question type, and print sessions\n",
    "\n",
    "num_multi_sessions = 500\n",
    "\n",
    "question_type_set = set(lme_dataset_df[\"question_type\"].unique())\n",
    "multi_session_lengths = {q_type: [] for q_type in question_type_set}\n",
    "session_lengths = {q_type: [] for q_type in question_type_set}\n",
    "\n",
    "for multi_session_idx in range(num_multi_sessions):\n",
    "    multi_session = lme_dataset_df[\"haystack_sessions\"].iloc[multi_session_idx]\n",
    "    question_type = lme_dataset_df[\"question_type\"].iloc[multi_session_idx]\n",
    "\n",
    "    # Create a unique Zep user and session for this multi-session.\n",
    "    # We only use one Zep session because it doesn't change things to use multiple Zep sessions\n",
    "    user_id = \"lme_experiment_user_\" + str(multi_session_idx)\n",
    "    session_id = \"lme_experiment_session_\" + str(multi_session_idx)\n",
    "\n",
    "    multi_session_lengths[question_type].append(len(multi_session))\n",
    "    for session in multi_session:\n",
    "        session_lengths[question_type].append(len(session))\n",
    "\n",
    "        # print(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aG64lm0f6Wvg",
    "outputId": "acb2c151-88ae-4e6b-99a4-2bb438b7d45b"
   },
   "outputs": [],
   "source": [
    "######## (Optional dataset investigation) Print out statistics by question type\n",
    "\n",
    "print(lme_dataset_option)\n",
    "\n",
    "# Iterate through each question type to print statistics about the possible multi-session lengths and session lengths\n",
    "for question_type in question_type_set:\n",
    "    print(f\"******Question Type: {question_type}\")\n",
    "\n",
    "    try:\n",
    "        print(\"made it\")\n",
    "        print(\"Number of multi-sessions:\", len(multi_session_lengths[question_type]))\n",
    "        print()\n",
    "        print(\"Max session length:\", max(session_lengths[question_type]))\n",
    "        print(\n",
    "            \"Avg session length:\",\n",
    "            sum(session_lengths[question_type]) / len(session_lengths[question_type]),\n",
    "        )\n",
    "        print(\"Min session length:\", min(session_lengths[question_type]))\n",
    "        print(\"Numer of sessions of length 0:\", session_lengths[question_type].count(0))\n",
    "    except:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
